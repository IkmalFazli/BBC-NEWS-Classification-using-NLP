# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c1lsAqLsQmHazk2EUNe8DT_Pu5FBxmCj
"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
from bbc_news_module import ModelCreation,Model_Evaluate
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import TensorBoard
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import pickle
import json
import os

log_dir = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
HEART_LOG_FOLDER_PATH = os.path.join(os.getcwd(),'logs',log_dir)
MODEL_SAVE_PATH = os.path.join(os.getcwd(),'model.h5')
TOKENIZER_PATH = os.path.join(os.getcwd(),'tokenizer_sentiment.json')
OHE_PATH = os.path.join(os.getcwd(),'ohe.pkl')
DATA_PATH = 'https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv'

"""# Step 1) Data Loading"""

df = pd.read_csv(DATA_PATH)

"""# Step 2) Data Inspection"""

print(df.head(10))
df.tail(10)
print(df.info())
print(df.describe())

df['category'].unique()

print(df['text'][0])
print(df['category'][0])

df.duplicated().sum()

"""# Step 3) Data Cleaning
- drop duplicates
"""

df = df.drop_duplicates()

"""#Step 4) Feature Selection
- nothing to select

# Step 5) Preprocessing
"""

# 1) Convert into lower case. -all already in lower case
# 2) Tokenizing

text = df['text'].values
category = df['category'].values

vocab_size = 10000
oov_token = 'OOV'

tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_token)

tokenizer.fit_on_texts(text) # to learn all of the words
word_index = tokenizer.word_index
print(word_index)

train_sequences = tokenizer.texts_to_sequences(text)


# 3) Padding & truncating
length_of_text = [len(i) for i in train_sequences] # list comprehension
print(np.median(length_of_text))  # to get the number of max length for padding

max_len = 340

padded_text = pad_sequences(train_sequences,maxlen=max_len,
                              padding='post',
                              truncating='post')

# 4) One Hot Encoding for the target

ohe = OneHotEncoder(sparse=False)
category = ohe.fit_transform(np.expand_dims(category,axis=-1))

# 5) Train test split

X_train,X_test,y_train,y_test = train_test_split(padded_text,category,
                                                 test_size=0.3,
                                                 random_state=123)

X_train = np.expand_dims(X_train,axis=-1)
X_test = np.expand_dims(X_test,axis=-1)

"""# Model Development"""

# Use LSTM layers, dropout, dense, input
# achieve >90% f1 score

mod = ModelCreation() # the model in module_NLP.py
model = mod.model_layer(num_node=128,drop_rate=0.2,output_node=5,
                        embed_dims=64,vocab_size=10000)

plot_model(model, to_file='model_plot.png', show_shapes=True,
           show_layer_names=True)

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics = ['acc'])

tensorboard_callback = TensorBoard(log_dir=HEART_LOG_FOLDER_PATH)

hist = model.fit(X_train,y_train,
                 epochs=50, batch_size=128,
                 validation_data=(X_test,y_test),
                 callbacks=[tensorboard_callback])

hist.history.keys()

plt.figure()
plt.plot(hist.history['loss'],'r--',label='Training loss')
plt.plot(hist.history['val_loss'],label='Validation loss')
plt.legend()
plt.plot()

plt.figure()
plt.plot(hist.history['acc'],'r--',label='Training acc')
plt.plot(hist.history['val_acc'],label='Validation acc')
plt.legend()
plt.plot()

Eva = Model_Evaluate()
Eva.EvaluateMymodel(model,X_test,y_test)
model.save(MODEL_SAVE_PATH)

token_json = tokenizer.to_json()

with open(TOKENIZER_PATH,'w') as file:
    json.dump(token_json,file)

with open(OHE_PATH,'wb') as file:
    pickle.dump(ohe,file)



plot_model(model, to_file='model_plot.png', show_shapes=True,
           show_layer_names=True)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs